# Chatbot Llama 3 com Ollama

Este projeto √© um chatbot baseado no modelo **Llama 3**, utilizando **Ollama** como backend para execu√ß√£o local do modelo de IA. A interface foi desenvolvida com **Flask**, permitindo intera√ß√£o via navegador.

## üöÄ Funcionalidades
- Chatbot local com modelo **Llama 3** rodando via **Ollama**.
- Interface web simples para interagir com o chatbot.
- Configura√ß√£o de API via vari√°veis de ambiente.
- Suporte a execu√ß√£o local e via Docker.

---

## üìã Pr√©-requisitos

Antes de come√ßar, instale os seguintes requisitos no seu sistema:

- **Python 3.10+**
- **pip** (gerenciador de pacotes do Python)
- **Docker** (opcional, caso queira rodar via container)
- **Ollama** (para rodar modelos LLM)

### Instalar Ollama
Se ainda n√£o tiver o **Ollama**, instale-o:
```bash
curl -fsSL https://ollama.com/install.sh | sh
```

Ap√≥s a instala√ß√£o, baixe o modelo **Llama 3**:
```bash
ollama pull llama3
```

---

## üì¶ Instala√ß√£o
### 1Ô∏è‚É£ Clonar o reposit√≥rio
```bash
git clone https://github.com/seu-usuario/chatbot-llama.git
cd chatbot-llama
```

### 2Ô∏è‚É£ Criar e ativar o ambiente virtual
```bash
python -m venv venv
source venv/bin/activate  # Linux/macOS
venv\Scripts\activate  # Windows
```

### 3Ô∏è‚É£ Instalar depend√™ncias
```bash
pip install -r requirements.txt
```

### 4Ô∏è‚É£ Configurar a URL da API Ollama
A API do Ollama deve estar rodando na porta padr√£o `11434`. Voc√™ pode definir a vari√°vel de ambiente `OLLAMA_API_URL` para apontar para a API do Ollama.

#### üìç Op√ß√£o 1: Definir vari√°vel de ambiente
```bash
export OLLAMA_API_URL="http://localhost:11434/v1/chat/completions"  # Linux/macOS
set OLLAMA_API_URL="http://localhost:11434/v1/chat/completions"  # Windows
```

#### üìç Op√ß√£o 2: Criar um arquivo `.env`
Crie um arquivo chamado **`.env`** na raiz do projeto e adicione:
```env
OLLAMA_API_URL=http://localhost:11434/v1/chat/completions
```

---

## ‚ñ∂Ô∏è Execu√ß√£o do projeto

### Rodando localmente
Certifique-se de que o **Ollama** est√° rodando e execute:
```bash
python src/app.py
```
O chatbot estar√° acess√≠vel via navegador em: [http://127.0.0.1:5000](http://127.0.0.1:5000)

---

## üê≥ Rodando via Docker
### 1Ô∏è‚É£ Construir a imagem Docker
```bash
docker build -t chatbot-llama .
```

### 2Ô∏è‚É£ Rodar o container
```bash
docker run -p 5000:5000 -e OLLAMA_API_URL="http://host.docker.internal:11434/v1/chat/completions" chatbot-llama
```

Acesse no navegador: [http://127.0.0.1:5000](http://127.0.0.1:5000)

‚ö†Ô∏è **Nota:** No Docker, use `http://host.docker.internal:11434/v1/chat/completions` para acessar o Ollama rodando no host.

---

### üîÑ Substituindo localhost pelo IP de um servidor Ollama remoto

Se voc√™ j√° possui um servidor Ollama rodando em outra m√°quina, basta substituir localhost pelo IP do servidor. Por exemplo:

export OLLAMA_API_URL="http://192.168.1.100:11434/v1/chat/completions"

Ou, se estiver usando Docker:

docker run -p 5000:5000 --env OLLAMA_API_URL="http://192.168.1.100:11434/v1/chat/completions" chatbot-llama

Isso permitir√° que o chatbot se conecte a um servidor Ollama remoto, caso seu ambiente local n√£o tenha capacidade suficiente para rod√°-lo.

## üìù Licen√ßa
Este projeto √© open-source sob a licen√ßa MIT.

---

üí° **Autor:** [Marco Gomes](https://github.com/mgsantos89)

